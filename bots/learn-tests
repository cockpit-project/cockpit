#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# This file is part of Cockpit.
#
# Copyright (C) 2017 Slavek Kabrda
#
# Cockpit is free software; you can redistribute it and/or modify it
# under the terms of the GNU Lesser General Public License as published by
# the Free Software Foundation; either version 2.1 of the License, or
# (at your option) any later version.
#
# Cockpit is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Cockpit; If not, see <http://www.gnu.org/licenses/>.

# The number of days of previous closed pull requests to learn from
SINCE_DAYS = 120

# The name and version of the training data
TRAINING_DATA = "tests-train-1.jsonl.gz"

import gzip
import os
import subprocess
import sys
import tempfile
import time

sys.dont_write_bytecode = True

import task
import learn.data

BOTS = os.path.dirname(os.path.realpath(__file__))
DATA = os.path.join(os.environ.get("TEST_DATA", BOTS), "images")

def run(training_data, verbose=False, dry=False, **kwargs):
    upload = [ os.path.join(BOTS, "image-upload"), "--state" ]

    # Default set of training data, retrieve it and use from DATA directory
    if not training_data:
        training_data = TRAINING_DATA
    if "/" not in training_data and not os.path.exists(training_data):
        if not dry:
            subprocess.check_call([ os.path.join(BOTS, "image-download"), "--state", training_data ])
        training_data = os.path.join(DATA, training_data)
    items = loader(training_data, verbose, dry)

    if not dry:
        subprocess.check_call(upload + [ training_data ])

    # Train for the first one
    path1 = train1(items, verbose)

    # Train for the second model
    path2 = train2(items, verbose)

    # Or do the upload if a real run
    if not dry:
        subprocess.check_call(upload + [ path1, path2 ])


# Load jsonl style data into items, or if no file specified
# then just run tests-data to retrieve live data
def loader(training_data, verbose, dry):
    items = [ ]

    # The input file exists
    exists = training_data and os.path.exists(training_data)

    # Dry mode just read a file
    if dry:
        proc = None
        if not exists:
            raise RuntimeError("learn-tests: the input file doesn't exist: {0}".format(training_data))
        inp = gzip.open(training_data, 'rb')
        if verbose:
            sys.stderr.write("Loading tests data\n")

    # Launch the test-data command directly, and save the output
    else:
        cmd = [ os.path.join(BOTS, "tests-data"), "--since" ]
        when = time.time() - 60 * 60 * 24 * SINCE_DAYS
        cmd.append(time.strftime("%Y-%m-%d", time.gmtime(when)))
        if exists:
            cmd += [ "--seed", training_data ]
        if verbose:
            if exists:
                sys.stderr.write("Loading existing tests data\n")
            cmd.append("--verbose")
        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE)
        inp = proc.stdout

    def failures_without_tracker(item):
        return item.get("status") == "failure" and item.get("tracker") is None

    # TODO: For now we avoid everything that already has a tracker
    # otherwise this results in too much data for our simple use
    # of the DBSCAN algorithm ... later we can preselect certain
    # values that have a tracker ... or use BIRCH
    try:
        items = list(learn.data.load(inp, only=failures_without_tracker, verbose=verbose))
    finally:
        if proc:
            proc.wait()
            if proc.returncode != 0:
                raise RuntimeError("tests-data command failed")
        inp.close()

    return items

def train1(items, verbose):
    import learn.learn1
    tokenizer = learn.learn1.Tokenizer1()
    tokenizer.parse(items, verbose)

    if verbose:
        sys.stderr.write("Found {0} tokens, {1} contexts and {2} tests names\n".format(
            len(tokenizer.tokens), len(tokenizer.contexts), len(tokenizer.tests)))
        sys.stderr.write("Training neural network with dataset of {0} samples\n".format(len(items)))

    network = learn.learn1.NNWithScaler1(tokenizer)
    network.train(items)

    return learn.learn1.save(DATA, network)

def train2(items, verbose):
    import learn.cluster
    model = learn.cluster.Model(verbose=verbose)
    model.train(items)

    directory = tempfile.mkdtemp(prefix="clusters-")
    if verbose:
        sys.stderr.write("{0}: Dumping clusters\n".format(directory))
    model.dump(directory)
    for filename in os.listdir(directory):
        task.attach(os.path.join(directory, filename))

    return learn.cluster.save(DATA, model)

if __name__ == '__main__':
    task.main(function=run, title="Learn from testing data", verbose=True)
